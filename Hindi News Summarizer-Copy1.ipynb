{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6775d8f0",
   "metadata": {},
   "source": [
    "### Objective of the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af089b",
   "metadata": {},
   "source": [
    "* The goal of text summarization is to create a shorter version of a longer text while retaining its key information and main ideas. \n",
    "* This can be useful in many scenarios, such as when someone needs to quickly understand the main points of an article, or when large amounts of text need to be processed efficiently. \n",
    "* In the case of Hindi articles, summarization can help people who are not fluent in Hindi to quickly grasp the essence of the text. \n",
    "* Additionally, summarization can be helpful for people who are short on time and want to get an overview of a longer text without having to read the entire thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca796786",
   "metadata": {},
   "source": [
    "### Why this Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f6f9d",
   "metadata": {},
   "source": [
    "* Time-Saving\n",
    "* Efficient Processing\n",
    "* Increases productivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c65dc6",
   "metadata": {},
   "source": [
    "### Model Considered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba032e",
   "metadata": {},
   "source": [
    "1. TF-IDF\n",
    "2. TEXT-RANK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96524b19",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1392565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "import math\n",
    "import re\n",
    "import operator\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb49f0",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788de9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "Stopwords = set(stopwords.words('hindi'))\n",
    "wordlemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d6fcae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(words):\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        lemmatized_words.append(wordlemmatizer.lemmatize(word))\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12c7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    symbol_list = r'[^a-zA-Z0-9\\s]'\n",
    "    text = re.sub(symbol_list,'',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c441d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq(words):\n",
    "    words = [word.lower() for word in words]\n",
    "    dict_freq = {}\n",
    "    words_unique = []\n",
    "    for word in words:\n",
    "        if word not in words_unique:\n",
    "            words_unique.append(word)\n",
    "    for word in words_unique:\n",
    "        dict_freq[word] = words.count(word)\n",
    "    return dict_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca1265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import tnt\n",
    "from nltk.corpus import indian\n",
    "train_data = indian.tagged_sents('hindi.pos')\n",
    "tnt_pos_tagger = tnt.TnT()\n",
    "tnt_pos_tagger.train(train_data)\n",
    "\n",
    "def pos_tagging(article):\n",
    "    \n",
    "    words = nltk.word_tokenize(article)\n",
    "    pos_tagged_words = tnt_pos_tagger.tag(words)\n",
    "    pos_tagged_noun_verb = []\n",
    "    \n",
    "    for word, tag in pos_tagged_words:\n",
    "        if tag == \"NN\" or tag == 'NNP' or tag == 'VM' or tag == 'VAUX':\n",
    "            pos_tagged_noun_verb.append(word)\n",
    "    return pos_tagged_noun_verb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1df131",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07920be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_score(word,sentence):\n",
    "    freq_sum = 0\n",
    "    word_frequency_in_sentence = 0\n",
    "    len_sentence = len(sentence)\n",
    "    for word_in_sentence in sentence.split():\n",
    "        if word == word_in_sentence:\n",
    "            word_frequency_in_sentence = word_frequency_in_sentence + 1\n",
    "    tf =  word_frequency_in_sentence/ len_sentence\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33467fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_score(no_of_sentences,word,sentences):\n",
    "    no_of_sentence_containing_word = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = remove_special_characters(str(sentence))\n",
    "        sentence = re.sub(r'\\d+', '', sentence)\n",
    "        sentence = sentence.split()\n",
    "        sentence = [word for word in sentence if word.lower() not in Stopwords and len(word)>1]\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        sentence = [wordlemmatizer.lemmatize(word) for word in sentence]\n",
    "        if word in sentence:\n",
    "            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n",
    "    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ed5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tfidf(dict_freq,word,sentences,sentence):\n",
    "    word_tfidf = []\n",
    "    tf = tf_score(word,sentence)\n",
    "    idf = idf_score(len(sentences),word,sentences)\n",
    "    return tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26ea80b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_importance(sentence,dict_freq,sentences):\n",
    "    sentence_score = 0\n",
    "    sentence = remove_special_characters(str(sentence)) \n",
    "    sentence = re.sub(r'\\d+', '', sentence)\n",
    "    pos_tagged_sentence = [] \n",
    "    no_of_sentences = len(sentences)\n",
    "    pos_tagged_sentence = pos_tagging(sentence)\n",
    "    for word in pos_tagged_sentence:\n",
    "        if word.lower() not in Stopwords and word not in Stopwords and len(word)>1: \n",
    "            word = word.lower()\n",
    "            word = wordlemmatizer.lemmatize(word)\n",
    "            sentence_score = sentence_score + word_tfidf(dict_freq,word,sentences,sentence)\n",
    "    return sentence_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5193135",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13745ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_file = st.file_uploader(\"Upload a Hindi text file (.txt)\", type=\"txt\")\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    file = open(uploaded_file, 'r',encoding='utf-8')\n",
    "    text = file.read()\n",
    "    tokenized_sentence = sent_tokenize(text)\n",
    "    # text = remove_special_characters(str(text)\n",
    "    text = re.sub(r'\\d+', '', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c7a726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words_with_stopwords = word_tokenize(text)\n",
    "tokenized_words = [word for word in tokenized_words_with_stopwords if word not in Stopwords]\n",
    "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "tokenized_words = [word.lower() for word in tokenized_words]\n",
    "tokenized_words = lemmatize_words(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fe743ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'उत्तर': 2, 'प्रदेश': 2, 'उत्तराखंड': 4, 'विधानसभा': 3, 'चुनाव': 5, 'के': 15, 'बीजेपी': 10, 'सोमवार': 1, 'उम्मीदवारों': 7, 'ऐलान': 7, 'रविवार': 1, 'केंद्रीय': 1, 'समिति': 1, 'बैठक': 3, 'नामों': 3, 'मुहर': 1, 'लगाई': 1, 'सूत्रों': 1, 'अनुसार': 1, 'यूपी': 4, 'नाम': 2, 'खबर': 1, 'थोड़ी': 1, 'देरी': 1, 'अमित': 1, 'शाह': 1, 'आवास': 1, 'चल': 2, 'सीनियर': 1, 'नेता': 2, 'मौजूद': 1, 'मीडिया': 1, 'रिपोर्ट्स': 1, 'मुताबिक': 1, 'जनवरी': 2, 'चाह': 1, 'पार्टी': 3, 'प्रत्याशियों': 1, 'पीछे': 1, 'सपा': 1, 'बसपा': 1, 'एक-एक': 1, 'लिस्ट': 4, 'जारी': 3, 'तैयारी': 1, 'पंजाब': 6, 'गोवा': 5, 'मणिपुर': 2, 'सीटों': 7, 'चरणों': 2, 'वोटिंग': 1, 'होगी': 1, 'पहला': 1, 'चरण-': 7, 'सीट': 3, 'फरवरी': 7, 'तीसरा': 1, 'चौथा': 1, 'पांचवा': 1, 'छठा': 1, 'मार्च': 4, 'सातवां': 1, 'उत्तराखंड-': 1, 'वोट': 1, 'डाले': 1, 'जाएंगे': 1, 'अलावा': 1, 'होंगे': 1, 'मतदान': 1, 'नतीजे': 1, 'आएंगे': 1, 'पिछले': 1, 'दिनों': 1, 'चुनावों': 1, 'पहली': 3, 'जेपी': 1, 'नड्डा': 1, 'बता': 1, 'दें': 1, 'अकाली': 2, 'दल': 2, 'गठबंधन': 1, 'लड़ती': 2, 'बाकि': 1, 'वर्तमान': 1, 'विधायक': 2, 'जिसमे': 1, 'नवजोत': 2, 'सिंह': 1, 'सिद्धू': 1, 'पत्नी': 1, 'कौर': 1, 'अमृतसर': 1, 'ईस्ट': 1, 'थीं': 1, 'वो': 1, 'छोड़कर': 1, 'कांग्रेस': 1, 'शामिल': 1, 'चुकी': 1, 'विधायकों': 1, 'बार': 1, 'भरोसा': 1, 'जताते': 1, 'टिकट': 1}\n",
      "Percentage of information to retain(in percent):50\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "word_freq = freq(tokenized_words)\n",
    "print(word_freq)\n",
    "input_user = int(input('Percentage of information to retain(in percent):'))\n",
    "no_of_sentences = int((input_user * len(tokenized_sentence))/100)\n",
    "print(no_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db04d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "sentence_with_importance = {} \n",
    "for sent in tokenized_sentence:\n",
    "    sentenceimp = sentence_importance(sent,word_freq,tokenized_sentence)\n",
    "    sentence_with_importance[c] = sentenceimp\n",
    "    c = c+1\n",
    "sentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69d2bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "sentence_no = []\n",
    "for word_prob in sentence_with_importance:\n",
    "    if cnt < no_of_sentences:\n",
    "        sentence_no.append(word_prob[0])\n",
    "        cnt = cnt+1\n",
    "    else:\n",
    "        break\n",
    "sentence_no.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52231d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "उत्तर प्रदेश और उत्तराखंड में होने वाले विधानसभा चुनाव के लिए बीजेपी सोमवार को उम्मीदवारों का ऐलान कर सकती है. रविवार को हुई बीजेपी की केंद्रीय चुनाव समिति की बैठक में नामों की मुहर लगाई गई है. सूत्रों के अनुसार यूपी के लिए बीजेपी 140 और उत्तराखंड के लिए 55 उम्मीदवारों के नाम का ऐलान कर सकती है. खबर है कि नामों के ऐलान में थोड़ी देरी हो सकती है. अमित शाह के आवास पर अभी एक और बैठक चल रही है. सभी सीनियर नेता बैठक में मौजूद हैं. मीडिया रिपोर्ट्स के मुताबिक, बीजेपी 20 जनवरी तक यूपी और उत्तराखंड के उम्मीदवारों का ऐलान करना चाह रही है. पार्टी पहले से ही प्रत्याशियों का नाम ऐलान करने में पीछे चल रही है. अभी तक यूपी में सपा और बसपा ने उम्मीदवारों का ऐलान कर दिया है. बीजेपी 17 और 19 जनवरी को भी एक-एक लिस्ट जारी करने की तैयारी में है. उत्तर प्रदेश, पंजाब, उत्तराखंड, गोवा, मणिपुर में\n",
      "होने जा रहे है. यूपी की 403 सीटों पर 7 चरणों में वोटिंग होगी. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 17:29:21.465 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\khuts\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "summary=\"\"\n",
    "for sentence in tokenized_sentence:\n",
    "    if count in sentence_no:\n",
    "        summary+= sentence+\" \"\n",
    "    count+=1\n",
    "# print(\"Summary:\")\n",
    "print(summary)\n",
    "\n",
    "if summary:\n",
    "        st.write(\"Summary:\", summary)\n",
    "else:\n",
    "    st.write(\"not good.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884dfd34",
   "metadata": {},
   "source": [
    "### TEXT RANK ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a1f7e",
   "metadata": {},
   "source": [
    "GloVe word embeddings are vector representation of words. \n",
    "These word embeddings will be used to create vectors for our sentences. \n",
    "We could have also used the Bag-of-Words or TF-IDF approaches to create features for our sentences, \n",
    "but these methods ignore the order of the words (and the number of features is usually pretty large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6cc6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ba8f217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in tokenized_sentence:\n",
    "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split()))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d7dcfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix\n",
    "sim_mat = np.zeros([len(tokenized_sentence), len(tokenized_sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08ee399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e07ccfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_sentence)):\n",
    "    for j in range(len(tokenized_sentence)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8a75d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.041036900926599625, 1: 0.04116904367455, 2: 0.045617230260577984, 3: 0.039560860052399095, 4: 0.0402508435710731, 5: 0.038696211050302715, 6: 0.0474254506325869, 7: 0.04181086322499163, 8: 0.0415147123943797, 9: 0.04837242057915085, 10: 0.03869621105030271, 11: 0.04329102319294459, 12: 0.010507212444514618, 13: 0.047043266436092794, 14: 0.04119214464565863, 15: 0.048481279766895896, 16: 0.03405237945946809, 17: 0.04128922712086917, 18: 0.04141951453722617, 19: 0.04956604192272435, 20: 0.0415147123943797, 21: 0.04547521536518254, 22: 0.047242650358291643, 23: 0.04477458493883706}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9d1b53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(19, 0.04956604192272435), (15, 0.048481279766895896), (9, 0.04837242057915085), (6, 0.0474254506325869), (22, 0.047242650358291643), (13, 0.047043266436092794), (2, 0.045617230260577984), (21, 0.04547521536518254), (23, 0.04477458493883706), (11, 0.04329102319294459), (7, 0.04181086322499163), (8, 0.0415147123943797), (20, 0.0415147123943797), (18, 0.04141951453722617), (17, 0.04128922712086917), (14, 0.04119214464565863), (1, 0.04116904367455), (0, 0.041036900926599625), (4, 0.0402508435710731), (3, 0.039560860052399095), (5, 0.038696211050302715), (10, 0.03869621105030271), (16, 0.03405237945946809), (12, 0.010507212444514618)]\n"
     ]
    }
   ],
   "source": [
    "scoresrank =  sorted(scores.items(), key=operator.itemgetter(1),reverse=True)\n",
    "print(scoresrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5df1e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "पंजाब के लिए पहली लिस्ट में 17 और गोवा के लिए 29 नामों का ऐलान किया गया.\n",
      "गोवा और पंजाब में मतदान 4 फरवरी को है.\n",
      "बीजेपी 17 और 19 जनवरी को भी एक-एक लिस्ट जारी करने की तैयारी में है.\n"
     ]
    }
   ],
   "source": [
    "# Specify number of sentences to form the summary\n",
    "sn = 3\n",
    "# Generate summary\n",
    "for i in range(sn):\n",
    "    print(tokenized_sentence[scoresrank[i][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a82680",
   "metadata": {},
   "source": [
    "### Future Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d77d40",
   "metadata": {},
   "source": [
    "* Deployment\n",
    "* Improvement in algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0adb0080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9553677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655e87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4fc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6f839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be8ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451ef6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ad95f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
